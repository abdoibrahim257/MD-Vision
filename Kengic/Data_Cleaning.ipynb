{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abdoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Indiana Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emphysema, without focal airspace opacity, pneumothorax,, or nodule, is no acute pulmonary granuloma, are clear of the heart size, and mild cardiomegaly, with chronic interstitial opacities atelectasis.\n",
      "Emphysema, without focal airspace opacity, pneumothorax, or nodule, is no acute pulmonary granuloma, are clear of the heart size, and mild cardiomegaly, with chronic interstitial opacities' atelectasis.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import language_tool_python\n",
    "\n",
    "# Load spaCy model for tokenization and sentence segmentation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def add_punctuation_and_correct_grammar(text):\n",
    "    # Use spaCy to process the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize list to hold the corrected sentences\n",
    "    corrected_sentences = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        tokens = [token.text for token in sent]\n",
    "        corrected_sentence = \"\"\n",
    "        \n",
    "        # Add commas and conjunctions where needed\n",
    "        for i, token in enumerate(tokens):\n",
    "            corrected_sentence += token\n",
    "            if i < len(tokens) - 1:\n",
    "                next_token = tokens[i + 1]\n",
    "                \n",
    "                # Add a comma before conjunctions or coordinating conjunctions\n",
    "                if next_token.lower() in {\"and\", \"or\", \"but\", \"nor\", \"for\", \"so\", \"yet\"}:\n",
    "                    corrected_sentence += \",\"\n",
    "                \n",
    "                # Add a comma after specific medical terms\n",
    "                if token.lower() in {\"emphysema\", \"opacity\", \"pneumothorax\", \"nodule\", \"granuloma\", \"cardiomegaly\", \"atelectasis\"}:\n",
    "                    corrected_sentence += \",\"\n",
    "                \n",
    "                corrected_sentence += \" \"\n",
    "        \n",
    "        # Add period at the end of the sentence if not present\n",
    "        if not corrected_sentence.strip().endswith(\".\"):\n",
    "            corrected_sentence = corrected_sentence.strip() + \".\"\n",
    "        corrected_sentences.append(corrected_sentence)\n",
    "    \n",
    "    # Join corrected sentences into a single text\n",
    "    corrected_text = ' '.join(corrected_sentences)\n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the LanguageTool object\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "def correct_grammar(text):\n",
    "    # Check the text for errors\n",
    "    matches = tool.check(text)\n",
    "    \n",
    "    # Correct the text based on the matches found\n",
    "    corrected_text = language_tool_python.utils.correct(text, matches)\n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "text = \"emphysema without focal airspace opacity pneumothorax or nodule is no acute pulmonary granuloma are clear of the heart size and mild cardiomegaly with chronic interstitial opacities atelectasis\"\n",
    "corrected_text = add_punctuation_and_correct_grammar(text)\n",
    "print(corrected_text)\n",
    "\n",
    "\n",
    "\n",
    "# text = \"emphysema without focal airspace opacity pneumothorax or nodule is no acute pulmonary granuloma are clear of the heart size and mild cardiomegaly with chronic interstitial opacities atelectasis\"\n",
    "corrected_text = correct_grammar(corrected_text)\n",
    "print(corrected_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "tags = ['cardiac monitor',\n",
    " 'lymphatic diseases',\n",
    " 'pulmonary disease',\n",
    " 'osteophytes',\n",
    " 'foreign body', #ttshal\n",
    " 'dish', #ttshal\n",
    " 'aorta, thoracic',\n",
    " 'atherosclerosis',\n",
    " 'histoplasmosis',\n",
    " 'hypoventilation',\n",
    " 'catheterization, central venous',\n",
    " 'pleural effusions',\n",
    " 'pleural effusion',\n",
    " 'callus',\n",
    " 'sternotomy',\n",
    " 'lymph nodes',\n",
    " 'tortuous aorta',\n",
    " 'stent', #ttshal\n",
    " 'interstitial pulmonary edema',\n",
    " 'cholecystectomies', #ttshal\n",
    " 'neoplasm', #ttshal\n",
    " 'central venous catheter', #catheter olyla awy bs venous kteera\n",
    " 'pneumothorax',\n",
    " 'metastatic disease',\n",
    " 'vena cava, superior',\n",
    " 'cholecystectomy',\n",
    " 'scoliosis',\n",
    " 'subcutaneous emphysema',\n",
    " 'thoracolumbar scoliosis',\n",
    " 'spinal osteophytosis',\n",
    " 'pulmonary fibroses',\n",
    " 'rib fractures',\n",
    " 'sarcoidosis',\n",
    " 'eventration',\n",
    " 'fibrosis',\n",
    " 'spine',\n",
    " 'obstructive lung disease',\n",
    " 'pneumonitis',\n",
    " 'osteopenia',\n",
    " 'air trapping',\n",
    " 'demineralization',\n",
    " 'mass lesion',\n",
    " 'pulmonary hypertension',\n",
    " 'pleural diseases',\n",
    " 'pleural thickening',\n",
    " 'calcifications of the aorta',\n",
    " 'calcinosis',\n",
    " 'cystic fibrosis',\n",
    " 'empyema',\n",
    " 'catheter',\n",
    " 'lymph',\n",
    " 'pericardial effusion',\n",
    " 'lung cancer',\n",
    " 'rib fracture',\n",
    " 'granulomatous disease',\n",
    " 'chronic obstructive pulmonary disease',\n",
    " 'rib',\n",
    " 'clip',\n",
    " 'aortic ectasia',\n",
    " 'shoulder',\n",
    " 'scarring',\n",
    " 'scleroses',\n",
    " 'adenopathy',\n",
    " 'emphysemas',\n",
    " 'pneumonectomy',\n",
    " 'infection',\n",
    " 'aspiration',\n",
    " 'bilateral pleural effusion',\n",
    " 'bulla',\n",
    " 'lumbar vertebrae',\n",
    " 'lung neoplasms',\n",
    " 'lymphadenopathy',\n",
    " 'hyperexpansion',\n",
    " 'ectasia',\n",
    " 'bronchiectasis',\n",
    " 'nodule',\n",
    " 'pneumonia',\n",
    " 'right-sided pleural effusion',\n",
    " 'osteoarthritis',\n",
    " 'thoracic spondylosis',\n",
    " 'picc',\n",
    " 'cervical fusion',\n",
    " 'tracheostomies',\n",
    " 'fusion',\n",
    " 'thoracic vertebrae',\n",
    " 'catheters',\n",
    " 'emphysema',\n",
    " 'trachea',\n",
    " 'surgery',\n",
    " 'cervical spine fusion',\n",
    " 'hypertension, pulmonary',\n",
    " 'pneumoperitoneum',\n",
    " 'scar',\n",
    " 'atheroscleroses',\n",
    " 'aortic calcifications',\n",
    " 'volume overload',\n",
    " 'right upper lobe pneumonia',\n",
    " 'apical granuloma',\n",
    " 'diaphragms',\n",
    " 'copd',\n",
    " 'kyphoses',\n",
    " 'spinal fractures',\n",
    " 'fracture',\n",
    " 'clavicle',\n",
    " 'focal atelectasis',\n",
    " 'collapse',\n",
    " 'thoracotomies',\n",
    " 'congestive heart failure',\n",
    " 'calcified lymph nodes',\n",
    " 'edema',\n",
    " 'degenerative disc diseases',\n",
    " 'cervical vertebrae',\n",
    " 'diaphragm',\n",
    " 'humerus',\n",
    " 'heart failure',\n",
    " 'normal',\n",
    " 'coronary artery bypass',\n",
    " 'pulmonary atelectasis',\n",
    " 'lung diseases,interstitial',\n",
    " 'pulmonary disease,chronic obstructive',\n",
    " 'opacity',\n",
    " 'deformity',\n",
    " 'chronic disease',\n",
    " 'pleura',\n",
    " 'aorta',\n",
    " 'tuberculoses',\n",
    " 'hiatal hernia',\n",
    " 'scolioses',\n",
    " 'pleural fluid',\n",
    " 'malignancy',\n",
    " 'kyphosis',\n",
    " 'bronchiectases',\n",
    " 'congestion',\n",
    " 'discoid atelectasis',\n",
    " 'nipple',\n",
    " 'bronchitis',\n",
    " 'pulmonary artery',\n",
    " 'cardiomegaly',\n",
    " 'thoracic aorta',\n",
    " 'arthritic changes',\n",
    " 'pulmonary edema',\n",
    " 'vascular calcification',\n",
    " 'sclerotic',\n",
    " 'central venous catheters',\n",
    " 'catheterization',\n",
    " 'hydropneumothorax',\n",
    " 'aortic valve',\n",
    " 'hyperinflation',\n",
    " 'prostheses',\n",
    " 'pacemaker,artificial',\n",
    " 'bypass grafts',\n",
    " 'pulmonary fibrosis',\n",
    " 'multiple myeloma',\n",
    " 'postoperative period',\n",
    " 'cabg',\n",
    " 'right lower lobe pneumonia',\n",
    " 'granuloma',\n",
    " 'degenerative change',\n",
    " 'atelectasis',\n",
    " 'inflammation',\n",
    " 'effusion',\n",
    " 'cicatrix',\n",
    " 'tracheostomy',\n",
    " 'aortic diseases',\n",
    " 'sarcoidoses',\n",
    " 'granulomas',\n",
    " 'interstitial lung disease',\n",
    " 'infiltrates',\n",
    " 'displaced fractures',\n",
    " 'chronic lung disease',\n",
    " 'picc line',\n",
    " 'intubation,gastrointestinal',\n",
    " 'lung diseases',\n",
    " 'multiple pulmonary nodules',\n",
    " 'intervertebral disc degeneration',\n",
    " 'pulmonary emphysema',\n",
    " 'spine curvature',\n",
    " 'fibroses',\n",
    " 'chronic granulomatous disease',\n",
    " 'degenerative disease',\n",
    " 'atelectases',\n",
    " 'ribs',\n",
    " 'pulmonary arterial hypertension',\n",
    " 'edemas',\n",
    " 'pectus excavatum',\n",
    " 'lung granuloma',\n",
    " 'plate-like atelectasis',\n",
    " 'enlarged heart',\n",
    " 'hilar calcification',\n",
    " 'heart valve prosthesis',\n",
    " 'tuberculosis',\n",
    " 'old injury',\n",
    " 'patchy atelectasis',\n",
    " 'histoplasmoses',\n",
    " 'exostoses',\n",
    " 'mastectomies',\n",
    " 'right atrium',\n",
    " 'large hiatal hernia',\n",
    " 'hernia, hiatal',\n",
    " 'aortic aneurysm',\n",
    " 'lobectomy',\n",
    " 'spinal fusion',\n",
    " 'spondylosis',\n",
    " 'ascending aorta',\n",
    " 'granulomatous infection',\n",
    " 'fractures, bone',\n",
    " 'calcified granuloma',\n",
    " 'degenerative joint disease',\n",
    " 'intubation, intratracheal',\n",
    " 'others']\n",
    "\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#read indiana dataset \n",
    "df = pd.read_csv('./indiana_reports.csv')\n",
    "\n",
    "#to capture\n",
    "measurement = re.compile(r'(\\d+(.\\d+)?)( )?((cm|mm)?( )?(x) (\\d+(.\\d+)?) )?(cm|mm)')\n",
    "ratio = re.compile(r'(\\d+(.\\d+)\\/)')\n",
    "rankNumbers = re.compile(r'[0-9](st|nd|rd|th)', re.I)\n",
    "words = re.compile(r'(day|film|recommend|prior|comparison|compare|image|T6|T8|T11|T12|low|right|left|mid|status|patient|exam|please|unchanged)', re.I)\n",
    "intact = re.compile(r'((?<= )( )?(is|are) intact)|((?<=  )(is|are) unremarkable)')\n",
    "#create a new dataframe df2\n",
    "df2 = pd.DataFrame()\n",
    "# df2['uid'] = df['uid']\n",
    "\n",
    "df['findings'] = df['findings'].str.replace(r'XXXX', '',regex = True)\n",
    "df['impression'] = df['impression'].str.replace(r'(XXXX\\.|XXXX)', '',regex = True)\n",
    "\n",
    "#remove any list numbu2. 3. and so on\n",
    "df['findings'] = df['findings'].str.replace(r'([0-9](\\.))|(^[0-9](\\.))', '',regex=True)\n",
    "df['impression'] = df['impression'].str.replace(r'([0-9](\\.))|(^[0-9](\\.))', '',regex=True) \n",
    "\n",
    "#remove comas\n",
    "df['findings'] = df['findings'].str.replace(r',|-|:|<>|\\\\', '',regex=True)\n",
    "df['impression'] = df['impression'].str.replace(r',|-', '',regex=True)\n",
    "\n",
    "#split each to list of sentences \n",
    "df['findings'] = df['findings'].map(lambda x: str(x).lower().split('.'))\n",
    "df['impression'] = df['impression'].map(lambda x: str(x).lower().split('.'))\n",
    "\n",
    "\n",
    "df['findings'] = df['findings'].apply(lambda x: [sentence for sentence in x if not (intact.search(sentence) or words.search(sentence) or rankNumbers.search(sentence) or measurement.search(sentence) or ratio.search(sentence))])\n",
    "df['impression'] = df['impression'].apply(lambda x: [sentence for sentence in x if not (intact.search(sentence) or words.search(sentence) or rankNumbers.search(sentence) or measurement.search(sentence) or ratio.search(sentence))])\n",
    "\n",
    "#loop on the each senctence in the list of sentences and remove any remaining numbers from the sentence \n",
    "df['findings'] = df['findings'].apply(lambda x: [re.sub(r'\\d+', '', sentence) for sentence in x])\n",
    "df['impression'] = df['impression'].apply(lambda x: [re.sub(r'\\d+', '', sentence) for sentence in x])\n",
    "\n",
    "#remove any empty sentences\n",
    "df['findings'] = df['findings'].apply(lambda x: [sentence for sentence in x if sentence.strip()])\n",
    "df['impression'] = df['impression'].apply(lambda x: [sentence for sentence in x if sentence.strip()])\n",
    "\n",
    "df2['imgID']= df['uid']\n",
    "\n",
    "df2['captions'] = df['findings'] + df['impression']\n",
    "\n",
    "df2['captions'] = df2['captions'].apply(lambda x: [sentence.split() for sentence in x if sentence != 'nan'])\n",
    "df2['captions'] = df2['captions'].apply(lambda x: [sentence for sentence in x if len(sentence) >2])\n",
    "# remove row with empty list\n",
    "df2 = df2[df2['captions'].map(len) > 0]\n",
    "\n",
    "df2.to_csv('indiana_reports_cleaned3.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df3 = df2.explode(\"captions\")\n",
    "# df3.explode(\"captions\")\n",
    "# #save df2 in csv \n",
    "# # split sentence and remove any row that has words that are <= 2\n",
    "# df3 = df3['captions'].map(lambda x: str(x).split())\n",
    "# # remove any row of size <= 2 \n",
    "# df3 = df3[df3.map(len) > 2]\n",
    "\n",
    "df3.to_csv('indiana_reports_cleaned.csv', index=False)\n",
    "# print(df3.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read\n",
    "df = pd.read_parquet('D:/GAM3A/5-Senior02/GP/KENGIC/MIMIC-medical-report/data/train-00000-of-00001-0dc3c7ebb0311aec.parquet')\n",
    "formatted_df = pd.DataFrame()\n",
    "#split the text given in to sentences\n",
    "#remove the following from findings and impression\n",
    "# any ___\n",
    "formatted_df['FINDINGS'] = df['FINDINGS'].str.replace(r'___', '', regex = True)\n",
    "formatted_df['IMPRESSION'] = df['IMPRESSION'].str.replace(r'___', '', regex = True)\n",
    "\n",
    "# any Dr.\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].str.replace(r'Dr.', '', regex = True)\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].str.replace(r'Dr.', '', regex = True)\n",
    "\n",
    "# any time formats ex: at 12:00 / at floating numbers\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].str.replace(r'(at \\d{1,2}:\\d{1,2})|(\\d{1,2}:\\d{1,2})', '', regex = True)\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].str.replace(r'(at \\d{1,2}:\\d{1,2})|(\\d{1,2}:\\d{1,2})', '', regex = True)\n",
    "\n",
    "# any p.m/a.m/am/pm\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].str.replace(r'( am )|( pm )|( p\\.m)|( a\\.m)', '', regex = True)\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].str.replace(r'( am )|( pm )|( p\\.m)|( a\\.m)', '', regex = True)\n",
    "\n",
    "# remove floating numbers followed by measurements ex: 12.5\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].str.replace(r'\\d+\\.\\d+', '', regex = True)\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].str.replace(r'\\d+\\.', '', regex = True)\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].str.replace(r'\\d+\\.\\d+', '', regex = True)\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].str.replace(r'\\d+\\.', '', regex = True)\n",
    "\n",
    "#remove any cm mm inch\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].str.replace(r'( cm)|( mm)', '', regex = True)\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].str.replace(r'( cm)|( mm)', '', regex = True)\n",
    "\n",
    "# remove any 1.,2.,3.,etc.\n",
    "#done in the above step\n",
    "\n",
    "# remove , =\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].str.replace(r',|=', '', regex = True)\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].str.replace(r',|=', '', regex = True)\n",
    "\n",
    "#remove any numbers\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].str.replace(r'\\d+', '', regex = True)\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].str.replace(r'\\d+', '', regex = True)\n",
    "\n",
    "#remove any \\n\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].str.replace(r'\\n', '', regex = True)\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].str.replace(r'\\n', '', regex = True)\n",
    "\n",
    "#split each paragraph on .\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].map(lambda x: str(x).split('.'))\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].map(lambda x: str(x).split('.'))\n",
    "\n",
    "#remove empty strings\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].map(lambda x: [i.split() for i in x if i != ''])\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].map(lambda x: [i.split() for i in x if i != ''])\n",
    "\n",
    "#check for since, through, by, on,\n",
    "#make every token a lower case \n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].apply(lambda x: [[word.lower() for word in sentence] for sentence in x])\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].apply(lambda x: [[word.lower() for word in sentence] for sentence in x])\n",
    "\n",
    "\n",
    "# #remove at ; however, new, from the sentence \n",
    "toRemove = ['at', 'however', 'new', 'from',';']\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].apply(lambda x: [[word for word in sentence if word not in toRemove] for sentence in x])\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].apply(lambda x: [[word for word in sentence if word not in toRemove] for sentence in x])\n",
    "\n",
    "\n",
    "#remove sentence with through, since, submitted, unchanged, compared, comparison, previous, prior,increase, decrease,increased, decreased,\n",
    "#findings, film, PICC, yesterday, today, SVC, tube,  \n",
    "toRemoveSentence = ['through', 'since', 'submitted', 'unchanged', 'compared', 'comparison', 'previous', 'prior', 'increase', 'decrease', 'increased', 'decreased', 'findings', 'film', 'picc', 'yesterday', 'today', 'svc', 'tubes']\n",
    "formatted_df['FINDINGS'] = formatted_df['FINDINGS'].apply(lambda x: [sentence for sentence in x if not any(word in sentence for word in toRemoveSentence)])\n",
    "formatted_df['IMPRESSION'] = formatted_df['IMPRESSION'].apply(lambda x: [sentence for sentence in x if not any(word in sentence for word in toRemoveSentence)])\n",
    "\n",
    "finalDf = pd.DataFrame()\n",
    "finalDf['captions'] = formatted_df['FINDINGS'] + formatted_df['IMPRESSION']\n",
    "\n",
    "# remove ['as','above'],['status','quo']\n",
    "toRemoveSentence = ['above', 'quo']\n",
    "finalDf['captions'] = finalDf['captions'].apply(lambda x: [sentence for sentence in x if (not any(word in sentence for word in toRemoveSentence) and len(sentence) > 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  [[bla, bla, bla], [bla2, bla2, bla2], [bla3, b...\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe with list of list of words in row\n",
    "temp = [\n",
    "   [ [[\"bla\", \"bla\", \"bla\"],\n",
    "    [\"bla2\", \"bla2\", \"bla2\"],\n",
    "    [\"bla3\", \"bla3\", \"bla3\"]]]\n",
    "]\n",
    "temp = pd.DataFrame(temp)\n",
    "print(temp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16763\n"
     ]
    }
   ],
   "source": [
    "#split lists to row\n",
    "# new = finalDf.explode('captions')\n",
    "newList = df3['captions'].tolist()\n",
    "print(len(newList))\n",
    "# Convert each inner list to a tuple and add them to a set\n",
    "# print(type(newList))\n",
    "unique_ref = set()\n",
    "for x in newList:\n",
    "    if isinstance(x, list):\n",
    "        t = tuple(x)\n",
    "        unique_ref.add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "sw = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4642\n",
      "['soft', 'tissues', 'within', 'normal', 'limits']\n",
      "['soft', 'tissues', 'within', 'normal', 'limits']\n"
     ]
    }
   ],
   "source": [
    "#convert from set of tuple to list of list\n",
    "unique_ref = [list(x) for x in unique_ref]\n",
    "print(len(unique_ref))\n",
    "print(unique_ref[0])\n",
    "#wrie to csv\n",
    "\n",
    "# sw = stopwords.words('english')\n",
    "#remove stopwords\n",
    "#convert list of lists to dataframe\n",
    "for i in range(len(unique_ref)):\n",
    "    x = unique_ref[i]\n",
    "    y = []\n",
    "    for j in x:\n",
    "        if j not in sw:\n",
    "            y.append(j)\n",
    "    unique_ref[i] = y\n",
    "    \n",
    "print(unique_ref[0])\n",
    "\n",
    "with open('mimic_reports_cleaned.csv', 'w') as f:\n",
    "    for item in unique_ref:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "    \n",
    "# df = pd.DataFrame(unique_ref, columns=['captions'], index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
